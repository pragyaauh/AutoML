{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMModel,LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading csv\n",
    "df=pd.read_csv(\"train_auto.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value analysis\n",
    "(df.isnull().sum()/len(df))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping na values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the missing percentage at max is 6%, we can drop these rows, however while fine tuning the model we can try to impute this values using IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.TARGET_FLAG.value_counts()/len(df))*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximately 30% of audience has filled for claims at some point or another.Also the dataset is imbalanced as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distibution of target amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(4.5,4.5)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=df[df[\"TARGET_AMT\"]!=0].TARGET_AMT).set_title('TARGET_AMT distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are a lot of outliers present so we can visualize the same distribution on amounts greater than 20,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=df[df[\"TARGET_AMT\"]>20000].TARGET_AMT).set_title('TARGET_AMT distribution greater than 20,000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall amount distribution of all claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting values where claims have been made\n",
    "df1=df[df['TARGET_AMT']!=0]\n",
    "#customized bins for count distribution\n",
    "bins = [0,1500,5000,7500,10000,100000]\n",
    "df1 = df1.groupby(pd.cut(df1[df1[\"TARGET_AMT\"]!=0].TARGET_AMT, bins=bins)).TARGET_AMT.count()\n",
    "df1.plot(kind='bar',title='Target amount distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above graph it is observable that most of the insurance claims lie between 1500-5000 and most less than 200 people make the heighest claims of >10K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the income and other monetary columns and plotting income distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the columns by replacing \",\",\"$\" and and converting it into integer\n",
    "cols= ['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM']\n",
    "for c in cols:\n",
    "    df[c] = df[c].str.replace(',', '')\n",
    "    df[c] = df[c].str.replace('$', '')\n",
    "    df[c]=df[c].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df[df[\"TARGET_FLAG\"]!=0]\n",
    "ax = sns.boxplot(x=df2['INCOME']).set_title('Income distribution of individuals making claims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of claim amount wrt the income and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(7,4.5)})\n",
    "\n",
    "sns.scatterplot(data=df2, x=\"INCOME\", y=\"TARGET_AMT\",hue=\"AGE\", size=\"AGE\",sizes=(20, 200)).set_title(\"Distribution of claim amount wrt the income and age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest insurance claims come from individuals within the bracket [0,150,000] and not the top earners.\n",
    "#### Their Age ranges from 20-40. There are some points eg target_amt 80000 made by a 20 year old,which could be suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df2, y=\"TARGET_AMT\", x=\"TRAVTIME\",hue=\"CAR_USE\", size=\"CAR_USE\",sizes=(20, 200)).set_title('Travel time versus target amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most claims are moade by commercial vehicals as compared to private.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=df2['CAR_AGE']).set_title('Car age when claim was made')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cars between range 1-10 are most likely to make insurance claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of car type and area wrt claim made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(24,24)})\n",
    "\n",
    "g = sns.catplot(hue=\"URBANICITY\", x=\"CAR_TYPE\", col=\"TARGET_FLAG\",\n",
    "                data=df, kind=\"count\")\n",
    "g.set_xticklabels(rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The ratio of suv to minivans are higher when the claims are made as compared to when they are not\n",
    "##### Significantly higher claims are made in urban areas\n",
    "##### Most claims in the rural areas are made by SUV owners, whereas none are made by van and panel truck owners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(6,6)})\n",
    "sns.scatterplot(data=df, y=\"OLDCLAIM\", x=\"HOME_VAL\", hue=\"TARGET_FLAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of old claims are less than 10000 in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,  cmap=cmap, vmax=.3, center=0,mask=mask,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is observable that target flag is negatively correlated to home_val and income.It is positively correlated to claim frequency and mvr pts\n",
    "### Within the varibles there is a strong correlation between income,home_val and bluebook with car age.This could be taken into account while tuning logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "df=pd.get_dummies(df, columns = ['PARENT1','MSTATUS','SEX','EDUCATION','JOB','CAR_USE','CAR_TYPE','RED_CAR','REVOKED','URBANICITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution in X and y for training models\n",
    "X=df[df.columns[~df.columns.isin(['TARGET_FLAG','INDEX','TARGET_AMT'])]]\n",
    "y = df['TARGET_FLAG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining multiple models for classification\n",
    "lreg=LogisticRegression(C= 100, penalty= 'l2', solver= 'newton-cg')\n",
    "sgd=SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "gnb=GaussianNB()\n",
    "Lgb = LGBMClassifier(n_estimators=90, silent=False, random_state =94, max_depth=2,num_leaves=31,objective='binary')\n",
    "ada=AdaBoostClassifier()\n",
    "rfc_cv=RandomForestClassifier()\n",
    "\n",
    "#creating a for loop for testing all models\n",
    "models=[lreg,rfc_cv,sgd,neigh,gnb,Lgb,ada]\n",
    "for m in models:\n",
    "    print(m)\n",
    "    # adding smote to use representative sampling \n",
    "    steps = [('over', SMOTE()), ('model', m)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    # evaluate pipeline\n",
    "    for scoring in[\"recall\", \"precision\",\"accuracy\"]:\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "        scores = cross_val_score(pipeline, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        print(\"Model\", scoring, \" mean=\", scores.mean() , \"stddev=\", scores.std())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above pipeline we can observe that GaussianNB,LGBM and AdaBoost are the top three performers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tuning of LGBM was giving a run time error I will try to hypertune Adaboost, on recall score to successfully classify as many claims as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of ADA-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # define model\n",
    "model = AdaBoostClassifier()\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "print(\"baseline\", np.mean(cross_val_score(model, X, y, scoring='recall', cv=cv, n_jobs=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on original ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding smoteenn sampling to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resampling\n",
    "resample = SMOTEENN()\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Score: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the score is improved,possible due to proper sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(\"AdaBoost\",np.round(np.mean(scores),3))\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using grid search CV to estimate the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "ada=AdaBoostClassifier()\n",
    "#setting up the grid search parameters\n",
    "search_grid={'m__n_estimators':[200,500,1000,2000],\n",
    "             'm__learning_rate':[.001,.01,.1,.2]}\n",
    "#applying grid search on the pipeline\n",
    "search=GridSearchCV(pipeline,param_grid=search_grid,scoring='recall',n_jobs=1,cv=crossvalidation)\n",
    "search.fit(X,y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model with updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(learning_rate=0.01,n_estimators=500)\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "y_pred = cross_val_predict(pipeline, X, y,  cv=10, n_jobs=-1)\n",
    "CM = confusion_matrix(y, y_pred)\n",
    "TN = CM[0][0] \n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "print(\"AdaBoost\",np.round(np.mean(scores),3))\n",
    "print(\"      Positives: \", 100*round(FN/(TP+FN),2), \"% misclassifed     \", FN, '/',TP+FN)\n",
    "print(\"      Negatives: \", 100*round(FP/(TN+FP),2), \"% misclassifed     \",FP, '/',TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the performance of the model has improved from the baseline.However,it is not significantly higher than that obtained after re-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature columns\n",
    "features =X.columns\n",
    "#defining model\n",
    "model = AdaBoostClassifier(learning_rate=0.01,n_estimators=500)\n",
    "model.fit(X,y)\n",
    "#storing feature importance from sklearn\n",
    "importances = model.feature_importances_\n",
    "#sorting the feature importance\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,20)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD_CLAIM,HOME_VAL,Urban/Rural are the most important features for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing model to pickle and using it to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "with open(pkl_filename, append'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Xtest csv and performing basic cleaning operations before making predictions\n",
    "Xtest=pd.read_csv(\"test_auto.csv\")\n",
    "#dropping target columns\n",
    "Xtest=Xtest[Xtest.columns[~Xtest.columns.isin(['TARGET_FLAG','INDEX','TARGET_AMT'])]]\n",
    "#dropping na values\n",
    "Xtest.dropna(inplace=True)\n",
    "#converting monetary columns into int after cleaning\n",
    "cols= ['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM']\n",
    "for c in cols:\n",
    "    Xtest[c] = Xtest[c].str.replace(',', '')\n",
    "    Xtest[c] = Xtest[c].str.replace('$', '')\n",
    "    Xtest[c]=Xtest[c].astype(int)\n",
    "#label encoding columns\n",
    "Xtest=pd.get_dummies(Xtest, columns = ['PARENT1','MSTATUS','SEX','EDUCATION','JOB','CAR_USE','CAR_TYPE','RED_CAR','REVOKED','URBANICITY'])\n",
    "\n",
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting predicitions into dataframe\n",
    "Ypredict = pickle_model.predict(Xtest)\n",
    "Ypredict=pd.DataFrame(Ypredict)\n",
    "Xtest['TARGET_FLAGS'] = Ypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypredict.rename(columns={ Ypredict.columns[0]: \"TARGET_PREDICTIONS\" }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypredict.to_csv(r'/home/jovyan/survey/Descartes/TARGET_PREDICTIONS.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far the predictions were made by dropping the na rows,however we canimpute the same by using Bayesian imputer and Categorical Imputer from Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning can also be done on other better performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
